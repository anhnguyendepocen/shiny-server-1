---
title: Introduction
author: Doom Lab
date: '2018-05-03'
slug: introduction
showDate: false
url: /introduction/
categories: []
tags: []
---

Effect sizes broadly fall into two categories:
  
1) Mean differences: normally represented by *Z* or *d*, usually the standardized difference between two means.

2) Variance overlap: normally representated by eta/omega, r-squared, or V, usually the amount of variance in the DV accounted for by the IV(s). 
  
Overall, we can think about effect sizes as a measure of the *magnitude* of an phenomenon. Unlike *p*-values, effect size is not as confounded by sample size. Effect sizes supplement *p*-values by providing this critical information. For example, we might want to know how much did grades improve after an intervention? Or to what degree were symptoms reduced after a treatment? 

Another benefit of providing effect sizes in research articles is the ability to estimate sample size for future studies. We can use the effect size(s) of previous research to plan the sample size for a new study. However, previous research shows that effect sizes are not perfect and can be slightly biased (too high). Therefore, MOTE also allows you to calculate the confidence interval of the effect size. With the confidence intervals, we can look at the range of the estimated effect size (i.e., is it small around the effect size, which might indicate we have a pretty good idea of what the effect might be? or it is large and the effect size might be pretty variable?) and use the range to get a better estimate of sample size for future studies. Also, the confidence intervals can give you an idea if an effect is different from zero - that is, if the confidence interval does not include zero, the effect size might not include no effect. The interpretation of the *size* of the effect (small, medium, large) should be based on your own field of study. 

Confidence Intervals
